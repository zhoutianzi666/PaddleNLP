## ğŸš£â€â™‚ï¸ ä½¿ç”¨ PaddleNLP åœ¨ MTT S4000 ä¸‹è·‘é€š llama2-13b æ¨¡å‹é¢„è®­ç»ƒ ğŸš£
PaddleNLP åœ¨ MTTÂ®S4000([äº†è§£æ‘©å°”çº¿ç¨‹](https://www.mthreads.com/))ä¸Šå¯¹ llama2-13B æ¨¡å‹è¿›è¡Œäº†æ·±åº¦é€‚é…å’Œä¼˜åŒ–ï¼Œä¸‹é¢ç»™å‡ºè¯¦ç»†å®‰è£…æ­¥éª¤ã€‚

##  ğŸš€ å¿«é€Ÿå¼€å§‹ ğŸš€

### ï¼ˆ0ï¼‰åœ¨å¼€å§‹ä¹‹å‰ï¼Œæ‚¨éœ€è¦æœ‰ä¸€å° MTT S4000 æœºå™¨ï¼Œå¯¹æ­¤æœºå™¨çš„ç³»ç»Ÿè¦æ±‚å¦‚ä¸‹ï¼š

 | èŠ¯ç‰‡ç±»å‹ | å¡å‹å· | é©±åŠ¨ç‰ˆæœ¬ |
 | --- | --- | --- |
 | MTT | S4000 | 2.7.0 |


### ï¼ˆ1ï¼‰ç¯å¢ƒå‡†å¤‡ï¼š(è¿™å°†èŠ±è´¹æ‚¨5ï½15min æ—¶é—´)
1. æ‹‰å–é•œåƒ
```
# æ³¨æ„æ­¤é•œåƒä»…ä¸ºå¼€å‘ç¯å¢ƒï¼Œé•œåƒä¸­ä¸åŒ…å«é¢„ç¼–è¯‘çš„é£æ¡¨å®‰è£…åŒ…
docker pull core.harbor.ghidc.mthreads.com:30003/library/musa-paddle-dev:dev3.1.0-paddle-2.6-v13
```
2. å‚è€ƒå¦‚ä¸‹å‘½ä»¤å¯åŠ¨å®¹å™¨
```
docker run it  --env MTHREADS_VISIBLE_DEVICES=all --ipc=host --net=host --cap-add=SYS_PTRACE --shm-size=40g core.harbor.ghidc.mthreads.com:30003/library/musa-paddle-dev:dev3.1.0-paddle-2.6-v13
```
3. å®‰è£… paddle
```
# paddlepaddleã€é£æ¡¨ã€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæä¾›è¿ç®—åŸºç¡€èƒ½åŠ›
git clone git@github.com:PaddlePaddle/Paddle.git -b release-musa/2.6
git submodule update --init --recursive
bash build.sh
```
4. å…‹éš† PaddleNLP ä»“åº“ä»£ç ï¼Œå¹¶å®‰è£…ä¾èµ–
```
# PaddleNLPæ˜¯åŸºäºpaddlepaddleã€é£æ¡¨ã€çš„è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤§è¯­è¨€æ¨¡å‹(LLM)å¼€å‘åº“ï¼Œå­˜æ”¾äº†åŸºäºã€é£æ¡¨ã€æ¡†æ¶å®ç°çš„å„ç§å¤§æ¨¡å‹ï¼Œllama2-13Bæ¨¡å‹ä¹ŸåŒ…å«å…¶ä¸­ã€‚ä¸ºäº†ä¾¿äºæ‚¨æ›´å¥½åœ°ä½¿ç”¨PaddleNLPï¼Œæ‚¨éœ€è¦cloneæ•´ä¸ªä»“åº“ã€‚
git clone git@github.com:shang-mt/PaddleNLP.git -b mthreads-llama-13B
cd PaddleNLP
python -m pip install -r requirements.txt
python -m pip install -e .
```

### ï¼ˆ2ï¼‰è®­ç»ƒï¼š
1. å¤šæœºå¤šå¡è®­ç»ƒ

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œè®­ç»ƒï¼š
```bash
cd llm/
bash run_dist.sh 10.10.10.123 # å‡è®¾master ip ä¸º10.10.10.123ï¼Œåœ¨ä¸åŒèŠ‚ç‚¹ä¸Šæ‰§è¡Œæ­¤å‘½ä»¤
```
æˆåŠŸè¿è¡Œåï¼Œå¯ä»¥æŸ¥çœ‹åˆ°è®­ç»ƒæ—¥å¿—çš„ç”Ÿæˆï¼Œæ ·ä¾‹å¦‚ä¸‹ï¼š
```bash
/home/baidu_test/miniconda3/envs/sci-baidu/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[33m[2024-04-30 17:28:18,614] [ WARNING][0m - evaluation_strategy reset to IntervalStrategy.STEPS for do_eval is True. you can also set evaluation_strategy='epoch'.[0m
[2024-04-30 17:28:18,614] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
[32m[2024-04-30 17:28:18,614] [    INFO][0m - PP configs:{'micro_batch_size': 2, 'accumulate_steps': 256, 'schedule_mode': '1F1B', 'p2p_cache_shape': True, 'enable_partial_send_recv': False}, use master_grad: 1[0m
[33m[2024-04-30 17:28:18,614] [ WARNING][0m - In pipeline model, the evaluation also shares same setting with training. We will enforce that per_device_eval_batch_size=per_device_train_batch_size * gradient_accumulation_steps.[0m
[32m[2024-04-30 17:28:18,615] [    INFO][0m - using pipeline configs:{'delay_scale_loss': False, 'dp_comm_overlap': False, 'sharding_comm_overlap': False, 'enable_timer': False, 'release_gradients': False}[0m
/home/baidu_test/zhonghui03/PaddleNLP/paddlenlp/trainer/training_args.py:1107: UserWarning: For pipeline parallel with sharding, the sharding overlap and tensor fusion should be configured in pipeline_parallel_config."enable_stage1_tensor_fusion" and "enable_stage1_overlap" in sharding_parallel_config will be ignored.
  warnings.warn(
======================= Modified FLAGS detected =======================
FLAGS(name='FLAGS_selected_gpus', current_value='0', default_value='')
=======================================================================
I0430 17:28:18.616860 1117373 tcp_utils.cc:181] The server starts to listen on IP_ANY:44264
I0430 17:28:18.617025 1117373 tcp_utils.cc:130] Successfully connected to 10.3.5.1:44264
I0430 17:28:21.722956 1117373 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
I0430 17:28:21.724152 1117373 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2024-04-30 17:28:21,724] [    INFO] topology.py:358 - Total 8 pipe comm group(s) create successfully!
W0430 17:28:21.727821 1117373 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.7
W0430 17:28:21.730523 1117373 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.
[2024-04-30 17:28:28,398] [    INFO] topology.py:358 - Total 32 data comm group(s) create successfully!
I0430 17:28:28.399701 1117373 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2024-04-30 17:28:28,400] [    INFO] topology.py:358 - Total 16 model comm group(s) create successfully!
I0430 17:28:28.400249 1117373 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2024-04-30 17:28:28,400] [    INFO] topology.py:358 - Total 8 sharding comm group(s) create successfully!
I0430 17:28:28.400563 1117373 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
I0430 17:28:28.400646 1117373 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
I0430 17:28:28.400784 1117373 process_group_nccl.cc:129] ProcessGroupNCCL pg_timeout_ 1800000
[2024-04-30 17:28:28,401] [    INFO] topology.py:288 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 4, pp_degree: 4, dp_degree: 1, sep_degree: 1, mp_group: [0, 1],  sharding_group: [0, 2, 4, 6], pp_group: [0, 8, 16, 24], dp_group: [0], sep:group: None, check/clip group: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[32m[2024-04-30 17:28:28,402] [    INFO][0m -     +==============================================================================+
```
